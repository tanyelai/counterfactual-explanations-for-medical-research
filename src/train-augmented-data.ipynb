{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "\n",
    "### MODELS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "### sklearn utils\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (precision_score, recall_score, confusion_matrix,\n",
    "                             classification_report, f1_score)\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/your/path\"\n",
    "\n",
    "evaluation_dir = root_dir + \"/evaluations\"\n",
    "\n",
    "dataset = pd.read_excel(f\"{root_dir}/data/augmented_file.xlsx\")\n",
    "\n",
    "#dataset = entire_parameters.drop([\"T2_Parenchyma\", \"FLAIR_Parenchyma\",\"T1_Parenchyma\", \"T1CE_Parenchyma\", \"DWI_Parenchyma\", \"ADC_Parenchyma\"], axis=1)\n",
    "dataset[\"TUMOR_TYPE\"] = dataset[\"TUMOR_TYPE\"].apply(lambda x: \"MB\" if x == \"MEDULLOBLASTOMA\" else x)\n",
    "dataset[\"TUMOR_TYPE\"] = dataset[\"TUMOR_TYPE\"].apply(lambda x: \"EP\" if x == \"EPENDYMOMA\" else x)\n",
    "dataset[\"TUMOR_TYPE\"] = dataset[\"TUMOR_TYPE\"].apply(lambda x: \"PA\" if x == \"PILOCYTIC ASTROCYTOMA\" else x)\n",
    "dataset[\"TUMOR_TYPE\"] = dataset[\"TUMOR_TYPE\"].apply(lambda x: \"BG\" if x == \"GLIOMA\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in datasets:\n",
    "#     df[\"TUMOR_TYPE\"].replace({\"MEDULLOBLASTOMA\": \"MB\", \"EPENDYMOMA\": \"EP\", \"PILOCYTIC ASTROCYTOMA\": \"PA\", \"GLIOMA\": \"BG\"}, inplace=True)\n",
    "#     df.loc[(df[\"TUMOR_TYPE\"] != 'MB') & (df[\"TUMOR_TYPE\"] != 'EP'), \"TUMOR_TYPE\"] = np.nan\n",
    "#     df.dropna(subset=[\"TUMOR_TYPE\"], inplace=True)\n",
    "\n",
    "\n",
    "filtered_mb = dataset[dataset[\"TUMOR_TYPE\"] == \"MB\"].head(25)\n",
    "filtered_ep = dataset[dataset[\"TUMOR_TYPE\"] == \"EP\"].head(25)\n",
    "filtered_pa = dataset[dataset[\"TUMOR_TYPE\"] == \"PA\"].head(25)\n",
    "filtered_bg = dataset[dataset[\"TUMOR_TYPE\"] == \"BG\"].head(25)\n",
    "\n",
    "filtered_dataset = pd.concat([filtered_mb, filtered_ep, filtered_pa, filtered_bg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset[\"TUMOR_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## name dataframes for each case\n",
    "filtered_dataset.name = \"MB-EP-PA-BG\"\n",
    "\n",
    "datasets = [filtered_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create XGBoost instance with default hyper-parameters\n",
    "xgb_estimator = XGBClassifier(random_state=42)\n",
    "rf_estimator = RandomForestClassifier(random_state=42)\n",
    "dt_estimator = DecisionTreeClassifier(random_state=42)\n",
    "gbc_estimator = GradientBoostingClassifier(random_state=42)\n",
    "cb_estimator = CatBoostClassifier(verbose = False, random_state=42)\n",
    "ada_estimator = AdaBoostClassifier(random_state=42)\n",
    "svm_estimator = SVC(random_state=42, kernel='linear')\n",
    "lr_estimator = LogisticRegression(random_state=42)\n",
    "\n",
    "\n",
    "        \n",
    "xgb_estimator.name = 'XGBoost'\n",
    "rf_estimator.name = 'Random Forest'\n",
    "dt_estimator.name = 'Decision Tree'\n",
    "gbc_estimator.name = 'Gradient Boosting'\n",
    "cb_estimator.name = 'CatBoost'\n",
    "ada_estimator.name = 'AdaBoost'\n",
    "svm_estimator.name = 'SVM'\n",
    "lr_estimator.name = 'Logistic Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('model', lr_estimator)\n",
    "])\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_estimator), ('rf', rf_estimator), ('lr', lr_pipeline), ('cb', cb_estimator)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf.name = 'Voting Classifier'\n",
    "\n",
    "models = [xgb_estimator, rf_estimator, dt_estimator, gbc_estimator, cb_estimator, ada_estimator, svm_estimator, lr_estimator, voting_clf]\n",
    "random_states = [1,42,123,1234,12345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path, state_dir, results_path, data_name_dir, figures_path, matrices_path):\n",
    "    try:\n",
    "        os.mkdir(path) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(state_dir) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(results_path)\n",
    "    except OSError as error: \n",
    "        pass \n",
    "        \n",
    "    try:\n",
    "        os.mkdir(data_name_dir) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(figures_path)\n",
    "    except OSError as error: \n",
    "        pass \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(matrices_path) \n",
    "    except OSError as error: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Note: This is a helper function to print the feature importances for traditional ML models.\n",
    "#\n",
    "# Inputs:\n",
    "#   model: the trained model\n",
    "#   df: the dataframe used to train the model\n",
    "#   path: the main path to save the plot to\n",
    "############################################################################################################\n",
    "\n",
    "def print_feature_importances(model, df, path):\n",
    "    feature_imp = pd.Series(model.feature_importances_, index = df.drop('TUMOR_TYPE', axis=1).columns).sort_values(ascending = False)\n",
    "    sns.set_style('dark', {'axes.grid' : False})\n",
    "    plt.figure(figsize = (8, 8))\n",
    "    with sns.plotting_context(rc={\"axes.labelsize\":16, \"xtick.labelsize\":14, \"ytick.labelsize\":14}):\n",
    "        sns.barplot(x = feature_imp, y = feature_imp.index, palette = \"coolwarm\", alpha=1)\n",
    "    print(\"\\n\")\n",
    "    plt.xlabel(\"Relative scores\", fontsize = 13, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Features\", fontsize = 13, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_FeatureImportances.pdf'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_FeatureImportances.png'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################################\n",
    "# NOTE: This is a helper function to print the feature coefficients\n",
    "#       of the logistic regression and svm model.\n",
    "# \n",
    "# Inputs:\n",
    "#   model: the trained model\n",
    "#   df: the dataframe used to train the model\n",
    "#   path: the main path to save the plot to\n",
    "#\n",
    "#   This function can be used for any model that has a coef_ attribute, \n",
    "#   when feature_importances_ is not available.\n",
    "############################################################################\n",
    "\n",
    "def print_feature_coefficients(model, df, path):\n",
    "    \n",
    "    plt.figure(figsize = (12, 8))\n",
    "    features = df.drop('TUMOR_TYPE', axis=1).columns\n",
    "    importances = pd.DataFrame(data={\n",
    "        'Attribute': features,\n",
    "        'Importance': model.coef_[0]\n",
    "    })\n",
    "\n",
    "    importances[\"Importance\"] = importances[\"Importance\"].to_numpy().astype(np.float)\n",
    "    importances = importances.sort_values(by='Importance', ascending=False)\n",
    "    indices = np.argsort(importances[\"Importance\"])\n",
    "    sns.set_style('dark', {'axes.grid' : False})\n",
    "    with sns.plotting_context(rc={\"axes.labelsize\":16, \"xtick.labelsize\":14, \"ytick.labelsize\":14}):\n",
    "        plt.barh(range(len(indices)), importances[\"Importance\"], color = sns.cubehelix_palette(18, start=.5, rot=-.5), align='center')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.barh(range(len(indices)), importances[\"Importance\"], color = sns.cubehelix_palette(18, start=.5, rot=-.5), align='center')\n",
    "    plt.yticks(range(len(indices)), importances[\"Attribute\"], fontsize = 13)\n",
    "    plt.xlabel(\"Relative weight scores\", fontsize = 13, fontweight=\"bold\")\n",
    "\n",
    "    # SAVE BOTH PNG AND PDF\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_impTable.png'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_impTable.pdf'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "\n",
    "    plt.clf()\n",
    "    #plt.show()\n",
    "    ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a helper function to print the results of the model\n",
    "# Inputs:\n",
    "#   y_test: the true labels\n",
    "#   y_pred: the predicted labels\n",
    "#   df: the dataframe used to train the model\n",
    "#   model: the trained model\n",
    "#   path: the main path to save the plot to\n",
    "\n",
    "\n",
    "def print_results(y_test, y_pred, df, model, path):\n",
    "    print('Precision: %.4f' % precision_score(y_test, y_pred, average='macro'))\n",
    "    print('Recall: %.4f' % recall_score(y_test, y_pred, average='macro'))\n",
    "    print('F1 Score: %.4f' % f1_score(y_test, y_pred, average='macro'))\n",
    "    print('\\n')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('\\n')\n",
    "    labels = np.unique(y_test)\n",
    "    plt.figure(figsize = (11, 8))\n",
    "    with sns.plotting_context(rc={\"axes.labelsize\":16, \"xtick.labelsize\":24, \"ytick.labelsize\":24}):\n",
    "        table = sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                            xticklabels = labels,\n",
    "                            yticklabels = labels,\n",
    "                            annot=True, fmt='.0f', cmap='Purples',\n",
    "                            annot_kws={\n",
    "                            'fontsize': 24,\n",
    "                            'fontweight': 'bold'})\n",
    "\n",
    "    table.set_xlabel('\\nPredicted Values', fontdict=dict(weight='bold', size=24))\n",
    "    table.set_ylabel('Actual Values', fontdict=dict(weight='bold', size=24))\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    img_path = os.path.join(path, df.name + '_' + model.name + '_matrix.png') \n",
    "    pdf_path = os.path.join(path, df.name + '_' + model.name + '_matrix.pdf') \n",
    "    \n",
    "    plt.savefig(img_path, bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.savefig(pdf_path, bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a helper function to obtain the results of a trained model.\n",
    "#      It splits the data into train and test sets, trains the model, and prints the results.\n",
    "\n",
    "\n",
    "def exec_model(model, df, path, state):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"TUMOR_TYPE\"], axis = 1), df[\"TUMOR_TYPE\"], \n",
    "                                                        test_size = 0.35, \n",
    "                                                        stratify = df[\"TUMOR_TYPE\"], \n",
    "                                                        random_state = state)\n",
    "\n",
    "    if(model.name == 'XGBoost'):\n",
    "        model = XGBClassifier(random_state=42)\n",
    "        model.name = 'XGBoost'\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "    \n",
    "    if(model.name == 'Voting Classifier'):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        pipe = make_pipeline(StandardScaler(), model)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    if model.name == 'XGBoost':\n",
    "        y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "    scores = []\n",
    "    scores.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    \n",
    "    ################## CREATE FILE PATH IF NOT EXIST #######################\n",
    "    state_dir = os.path.join(path, \"random_state-\" + f\"{state}\")\n",
    "    data_name_dir = os.path.join(state_dir, df.name + f\"_rs{state}\")\n",
    "    results_path = os.path.join(path, \"results\")\n",
    "    figures_path = os.path.join(data_name_dir, \"figures\")\n",
    "    matrices_path = os.path.join(data_name_dir, \"confusion_matrices\")\n",
    "\n",
    "    create_dir(path, state_dir, results_path, data_name_dir, figures_path, matrices_path)\n",
    "\n",
    "    ######################################################################\n",
    "    # PRINT RESULTS\n",
    "    print_results(y_test, y_pred, df, model, matrices_path)\n",
    "\n",
    "    importance = np.zeros(X_train.columns.size,)\n",
    "\n",
    "    if model.name == 'Logistic Regression':\n",
    "        print_feature_coefficients(model, df, figures_path)\n",
    "    elif model.name == 'SVM':\n",
    "        print_feature_coefficients(model, df, figures_path)\n",
    "    elif model.name == 'Voting Classifier':\n",
    "        pass\n",
    "    else:\n",
    "        print_feature_importances(model, df, figures_path)\n",
    "        importance = np.array(model.feature_importances_)\n",
    "    \n",
    "    importance = np.reshape(importance, (1, X_train.columns.size))\n",
    "    importance_df = pd.DataFrame(data=importance, columns=X_train.columns)\n",
    "\n",
    "    return importance_df, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Note: This is main function that loops over the different datasets and models.\n",
    "#       It calls the exec_model function to train and evaluate the models.\n",
    "#       It also saves the results to a file.\n",
    "#       The results are saved in a directory called \"evaluation\" in the current working directory.\n",
    "####################################################################################################\n",
    "\n",
    "for state in random_states:\n",
    "    for data in datasets:\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        importances = pd.DataFrame(data=None, columns=data.drop([\"TUMOR_TYPE\"], axis = 1).columns)\n",
    "        score_table = pd.DataFrame({\"Models\": [\"XGB\", \"RF\", \"DT\", \"GB\", \"CB\", \"ADA\", \"SVM\", \"LR\", \"VOTING\"],\n",
    "                                    \"Precision\": np.nan,\n",
    "                                    \"Recall\": np.nan,\n",
    "                                    \"F1_Score\": np.nan})\n",
    "        \n",
    "        \n",
    "        for model in models:\n",
    "            print(\"\\n*********************************************************************************\" +\n",
    "                    \"\\n-------------- \" + data.name + \"  -  \" + model.name + \" --------------\\n\")\n",
    "            \n",
    "            importance_df, scores = exec_model(model, data, evaluation_dir, state)\n",
    "            precisions.append(scores[0])\n",
    "            recalls.append(scores[1])\n",
    "            f1_scores.append(scores[2])\n",
    "            importances = pd.concat([importances, importance_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "        score_table[\"Precision\"] = precisions\n",
    "        score_table[\"Recall\"] = recalls\n",
    "        score_table[\"F1_Score\"] = f1_scores\n",
    "    \n",
    "        score_table = pd.concat([score_table, importances], axis = 1)\n",
    "\n",
    "        print(score_table)\n",
    "        out_path = os.path.join(evaluation_dir, 'results', data.name + f'_RS{state}.xlsx')\n",
    "        score_table.to_excel(out_path, encoding='utf-8')\n",
    "        del score_table, importances\n",
    "\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
