## Unveiling the `what if?' enigma: Exploiting counterfactual explanations for medical research
#### What if we could know what might happen in alternative scenarios in the decision space?


### Introduction

Medical research and diagnostics have witnessed substantial advancements with the integration of machine learning algorithms. However, the growing complexity of these algorithms has often resulted in a lack of transparency and interpretability, which has impeded their acceptance and adoption in clinical practice. The interpretability of machine learning models is of utmost importance, especially when it comes to critical domains such as pediatric brain tumor diagnosis using MRI features in the posterior fossa.

### Motivation

The motivation behind this research is to overcome the challenges posed by the lack of human-friendly interpretability in existing machine learning algorithms for most of the medical domains. End-users, such as clinicians and patients, might exhibit greater interest in comprehending the real-world consequences of the ML model's predictions in their own context, rather than solely fixating on the intricacies of how the models reached those predictions. For instance, patients might not only desire to know if they are sick, but also seek advice on how to be healthy again. They are less concerned about understanding the decision-making process of either the clinician or the ML algorithm.


### Methodology

Our proposed method involves integrating post-hoc model-agnostic counterfactual explanations for the machine learning models used in the diagnosis of posterior fossa pediatric brain tumors. Counterfactual explanations are instances generated by the model that represent alternative scenarios, highlighting the changes required in the input features to achieve a different outcome. By analyzing these counterfactual explanations, clinicians can gain a deeper understanding of the ML model's outcomes and the potential consequences in alternative scenarios that might already exist in alternative worlds.

### Key Contributions

***1. Enhanced Interpretability:*** By incorporating counterfactual explanations, our approach provides clear and understandable insights into the MRI features' importance in diagnosing pediatric brain tumors.<br/>
***2. Validation of Predictions:*** The personalized and context-specific counterfactual explanations serve as validation for the model's predictions, instilling confidence in clinicians' decision-making process.<br/>
***3. Exploring Alternative Scenarios:*** Clinicians can explore alternative decision-making scenarios by analyzing the generated counterfactual explanations, facilitating a comprehensive assessment of patient-specific conditions.<br/>
***4. Preserving Statistical and Clinical Fidelity:*** The counterfactual explanations generated by our approach preserve the statistical and clinical fidelity of the original data in most cases, ensuring reliable and informative explanations. (However, still requires further studies to be confident.)<br/>
***5. Data Augmentation:*** This study also explores the potential use of counterfactuals for data augmentation, enhancing the robustness and generalization of the machine learning model.<br/>
***6. Alternative Approach Assessment:*** We investigate the viability of counterfactual explanations as an alternative approach for enhancing interpretability in medical research.

### Citation

If you use our work or find it relevant to your research, please consider citing our paper:
{will be available soon.}
